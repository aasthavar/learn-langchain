{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LCEL**: a declarative way to compose chains together\n",
    "\n",
    "Basics:\n",
    "1. **A unified interface**: every lcel object implements `Runnable` eiifccuherecdinterface (common invocation methods: `invoke`, `batch`, `stream`, `ainvoke`...)\n",
    "2. **Composition Primitives**: easy to compose chains, parallelize components, add fallbacks, dynamically configure chain internal, and more.\n",
    "\n",
    "Reasons to use:\n",
    "1. **streaming support**: best possible TTFT. stream from llm -> op_parser -> user. (incremental chunks of op at the same rate as the LLM provider outputs the raw tokens)\n",
    "2. **async support**: async API. handle many concurrent requests in the same server.\n",
    "3. **optimized parallel execution**\n",
    "4. **retries and fallbacks**: streaming for retries/fallbacks - not available yet\n",
    "5. **access intermediate steps**: with streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qu langchain langchain-core langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic example: prompt + model + output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a short joke about AI research:\n",
      "\n",
      "Why did the AI researcher cross the road? To get to the other side... and then analyze the data to optimize their crossing strategy.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a short joke about {topic}\"\n",
    ")\n",
    "\n",
    "llm = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    client=boto3.client(\"bedrock-runtime\"),\n",
    "    model_kwargs={\"temperature\": 0.0, \"max_tokens\":128}\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "print(chain.invoke({\"topic\": \"AI research\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### rag search example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install faiss-cpu --quiet\n",
    "\n",
    "import os, json\n",
    "with open(\"/home/ubuntu/config.json\") as file:\n",
    "    config = json.load(file)\n",
    "os.environ[\"COHERE_API_KEY\"] = config[\"cohere_api_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given context, which consists of two documents with the page contents \"harrison worked at kensho\" and \"bears like to eat honey\", the answer to the question \"where did harrison work?\" is that Harrison worked at Kensho.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import CohereEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import boto3\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=CohereEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt_template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "\n",
    "llm = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    client=boto3.client(\"bedrock-runtime\"),\n",
    "    model_kwargs={\"temperature\": 0.0, \"max_tokens\":128}\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"where did harrison work?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=config[\"langsmith_api_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given context, which consists of two documents with the page contents \"harrison worked at kensho\" and \"bears like to eat honey\", the answer to the question \"where did harrison work?\" is that Harrison worked at Kensho.\n"
     ]
    }
   ],
   "source": [
    "# check in Langsmith UI\n",
    "print(chain.invoke(\"where did harrison work?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given context, which consists of two documents with the page contents \"harrison worked at kensho\" and \"bears like to eat honey\", the answer to the question \"where did harrison work?\" is that Harrison worked at Kensho."
     ]
    }
   ],
   "source": [
    "stream = chain.stream(\"where did harrison work?\")\n",
    "# stream = chain.stream(\"what is the current state of the art in AGI research ?\")\n",
    "for chunk in stream:\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add fallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a short joke about {topic}\"\n",
    ")\n",
    "\n",
    "haiku_llm = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    client=boto3.client(\"bedrock-runtime\"),\n",
    "    # model_kwargs={\"temperature\": 0.0, \"max_tokens\":128}\n",
    "    model_kwargs={\"temperature\": 0.0, \"max_tokens_to_sample\":128} # intentionally done to test faillback\n",
    ")\n",
    "\n",
    "sonnet_llm  = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    client=boto3.client(\"bedrock-runtime\"),\n",
    "    model_kwargs={\"temperature\": 0.0, \"max_tokens\":128}\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "haiku_chain = prompt | haiku_llm | output_parser\n",
    "sonnet_chain = prompt | sonnet_llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error raised by bedrock service: An error occurred (ValidationException) when calling the InvokeModel operation: Malformed input request: #: subject must not be valid against schema {\"required\":[\"messages\"]}#: extraneous key [max_tokens_to_sample] is not permitted, please reformat your input and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationException\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_community/llms/bedrock.py:503\u001b[0m, in \u001b[0;36mBedrockBase._prepare_input_and_invoke\u001b[0;34m(self, prompt, system, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m     text, body \u001b[38;5;241m=\u001b[39m LLMInputOutputAdapter\u001b[38;5;241m.\u001b[39mprepare_output(\n\u001b[1;32m    506\u001b[0m         provider, response\n\u001b[1;32m    507\u001b[0m     )\u001b[38;5;241m.\u001b[39mvalues()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/botocore/client.py:553\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/botocore/client.py:1009\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValidationException\u001b[0m: An error occurred (ValidationException) when calling the InvokeModel operation: Malformed input request: #: subject must not be valid against schema {\"required\":[\"messages\"]}#: extraneous key [max_tokens_to_sample] is not permitted, please reformat your input and try again.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mhaiku_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtopic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAI research\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print(sonnet_chain.invoke({\"topic\": \"AI research\"}))\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/runnables/base.py:2075\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2075\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:166\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    163\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    165\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    175\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    543\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_community/chat_models/bedrock.py:260\u001b[0m, in \u001b[0;36mBedrockChat._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop:\n\u001b[1;32m    258\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m stop\n\u001b[0;32m--> 260\u001b[0m     completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_input_and_invoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43msystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatted_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(\n\u001b[1;32m    270\u001b[0m     generations\u001b[38;5;241m=\u001b[39m[ChatGeneration(message\u001b[38;5;241m=\u001b[39mAIMessage(content\u001b[38;5;241m=\u001b[39mcompletion))]\n\u001b[1;32m    271\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_community/llms/bedrock.py:510\u001b[0m, in \u001b[0;36mBedrockBase._prepare_input_and_invoke\u001b[0;34m(self, prompt, system, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m     text, body \u001b[38;5;241m=\u001b[39m LLMInputOutputAdapter\u001b[38;5;241m.\u001b[39mprepare_output(\n\u001b[1;32m    506\u001b[0m         provider, response\n\u001b[1;32m    507\u001b[0m     )\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 510\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by bedrock service: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    513\u001b[0m     text \u001b[38;5;241m=\u001b[39m enforce_stop_tokens(text, stop)\n",
      "\u001b[0;31mValueError\u001b[0m: Error raised by bedrock service: An error occurred (ValidationException) when calling the InvokeModel operation: Malformed input request: #: subject must not be valid against schema {\"required\":[\"messages\"]}#: extraneous key [max_tokens_to_sample] is not permitted, please reformat your input and try again."
     ]
    }
   ],
   "source": [
    "print(haiku_chain.invoke({\"topic\": \"AI research\"})) # will throw an error\n",
    "\n",
    "# print(sonnet_chain.invoke({\"topic\": \"AI research\"})) # will work perfectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the AI researcher's office so hot? The machine learning was overheating!\n"
     ]
    }
   ],
   "source": [
    "fallback_chain = haiku_chain.with_fallbacks([sonnet_chain])\n",
    "\n",
    "print(fallback_chain.invoke({\"topic\": \"AI research\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add configurables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a short joke about {topic}\"\n",
    ")\n",
    "\n",
    "haiku = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    client=boto3.client(\"bedrock-runtime\"),\n",
    "    # model_kwargs={\"temperature\": 0.0, \"max_tokens\":128}\n",
    "    model_kwargs={\"temperature\": 0.0, \"max_tokens_to_sample\": 128} # intentionally done to test faillback\n",
    ")\n",
    "\n",
    "sonnet  = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    client=boto3.client(\"bedrock-runtime\"),\n",
    "    model_kwargs={\"temperature\": 0.0, \"max_tokens\": 128}\n",
    ")\n",
    "\n",
    "claude2 = BedrockChat(\n",
    "    model_id=\"anthropic.claude-v2:1\",\n",
    "    client=boto3.client(\"bedrock-runtime\"),\n",
    "    model_kwargs={\"temperature\": 0.0, \"max_tokens\": 128}\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "configurable_llm = llm.configurable_alternatives(\n",
    "    ConfigurableField(id=\"llm\"),\n",
    "    default_key=\"claude2\",\n",
    "    haiku=haiku,\n",
    "    sonnet=sonnet,\n",
    ")\n",
    "\n",
    "configurable_chain = (\n",
    "    {\n",
    "        \"topic\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | configurable_llm\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a short joke about AI research:\\n\\nWhy did the AI researcher cross the road? To get to the other side... and then analyze the data to optimize their crossing strategy.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configurable_chain.invoke(\n",
    "    \"AI research\",\n",
    "    config={\n",
    "        \"llm\": \"haiku\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a short joke about AI research:\\n\\nWhy did the AI researcher cross the road? To get to the other side... and then analyze the data to optimize their crossing strategy.\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configurable_chain.invoke(\n",
    "    \"AI research\",\n",
    "    # config={\n",
    "    #     \"llm\": \"haiku\"\n",
    "    # }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a short joke about AI research:\\n\\nWhy did the AI researcher cross the road? To get to the other side... and then analyze the data to determine the optimal path for future crossings.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configurable_chain.invoke(\n",
    "    \"AI research\",\n",
    "    config={\n",
    "        \"llm\": \"sonnet\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine - tracing + streaming + fallbacks + configurables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import ( ConfigurableField, RunnablePassthrough )\n",
    "\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=config[\"langsmith_api_key\"]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a short joke about {topic}\"\n",
    ")\n",
    "\n",
    "haiku = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    client=boto3.client(\"bedrock-runtime\"),\n",
    "    model_kwargs={\"temperature\": 0.0, \"max_tokens\":128}\n",
    "    # model_kwargs={\"temperature\": 0.0, \"max_tokens_to_sample\": 128} # intentionally done to test faillback\n",
    ")\n",
    "\n",
    "sonnet  = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    client=boto3.client(\"bedrock-runtime\"),\n",
    "    model_kwargs={\"temperature\": 0.0, \"max_tokens\": 128}\n",
    ")\n",
    "\n",
    "claude2 = BedrockChat(\n",
    "    model_id=\"anthropic.claude-v2:1\",\n",
    "    client=boto3.client(\"bedrock-runtime\"),\n",
    "    model_kwargs={\"temperature\": 0.0, \"max_tokens\": 128}\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "llm = (\n",
    "    claude2\n",
    "    .with_fallbacks([sonnet])\n",
    "    .configurable_alternatives(\n",
    "        ConfigurableField(id=\"llm\"),\n",
    "        default_key=\"claude2\",\n",
    "        haiku=haiku,\n",
    "        sonnet=sonnet,\n",
    "    )\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"topic\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why can't AI researchers tell the difference between Halloween and Christmas? Because Oct 31 = Dec 25!"
     ]
    }
   ],
   "source": [
    "# chain.invoke(\"AI research\", config={\"llm\": \"haiku\"})\n",
    "response = chain.stream(\"AI research\", config={\"llm\": \"haiku\"})\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `stream`: stream back chunks of response\n",
    "- `invoke`: call the chain on an input\n",
    "- `batch`: call the chain on a list of inputs\n",
    "\n",
    "async methods:\n",
    "- `astream`: stream back chunks of response async\n",
    "- `ainvoke`: call the chain on an input async\n",
    "- `abatch`: call the chain on a list of inputs async\n",
    "- `astream_log`: stream back intermediate steps as they happen, in addition to the final response\n",
    "- `astream_events`:  stream events as they happen in chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "{'event': 'on_chain_start', 'run_id': 'e348e1ac-9a95-46a1-b74c-6ac75ffd7a16', 'name': 'RunnableSequence', 'tags': [], 'metadata': {}, 'data': {'input': 'AI research'}}\n",
      "----------------------------------------------------------------------\n",
      "{'event': 'on_chain_start', 'name': 'RunnableParallel<topic>', 'run_id': '4b6586f5-24fc-4cb2-95ee-c37176f06e95', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {}}\n",
      "----------------------------------------------------------------------\n",
      "{'event': 'on_chain_start', 'name': 'RunnablePassthrough', 'run_id': '26cd260f-f8aa-41d4-9996-602038031591', 'tags': ['map:key:topic'], 'metadata': {}, 'data': {}}\n",
      "----------------------------------------------------------------------\n",
      "{'event': 'on_chain_stream', 'name': 'RunnablePassthrough', 'run_id': '26cd260f-f8aa-41d4-9996-602038031591', 'tags': ['map:key:topic'], 'metadata': {}, 'data': {'chunk': 'AI research'}}\n",
      "----------------------------------------------------------------------\n",
      "{'event': 'on_chain_stream', 'name': 'RunnableParallel<topic>', 'run_id': '4b6586f5-24fc-4cb2-95ee-c37176f06e95', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'chunk': {'topic': 'AI research'}}}\n",
      "----------------------------------------------------------------------\n",
      "{'event': 'on_chain_end', 'name': 'RunnablePassthrough', 'run_id': '26cd260f-f8aa-41d4-9996-602038031591', 'tags': ['map:key:topic'], 'metadata': {}, 'data': {'input': 'AI research', 'output': 'AI research'}}\n",
      "----------------------------------------------------------------------\n",
      "{'event': 'on_chain_end', 'name': 'RunnableParallel<topic>', 'run_id': '4b6586f5-24fc-4cb2-95ee-c37176f06e95', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': 'AI research', 'output': {'topic': 'AI research'}}}\n",
      "----------------------------------------------------------------------\n",
      "{'event': 'on_prompt_start', 'name': 'ChatPromptTemplate', 'run_id': '635f71ce-2d67-49da-97e7-232421424bff', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'input': {'topic': 'AI research'}}}\n",
      "----------------------------------------------------------------------\n",
      "{'event': 'on_prompt_end', 'name': 'ChatPromptTemplate', 'run_id': '635f71ce-2d67-49da-97e7-232421424bff', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'input': {'topic': 'AI research'}, 'output': ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about AI research')])}}\n",
      "----------------------------------------------------------------------\n",
      "{'event': 'on_chain_start', 'name': 'RunnableWithFallbacks', 'run_id': '2bae6484-b098-45dd-b583-06856b72ed9b', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'input': ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about AI research')])}}\n",
      "----------------------------------------------------------------------\n",
      "{'event': 'on_chat_model_start', 'name': 'BedrockChat', 'run_id': 'b94eb4ff-ec35-491f-91e0-a3d1365e2d11', 'tags': [], 'metadata': {}, 'data': {'input': {'messages': [[HumanMessage(content='tell me a short joke about AI research')]]}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "{'event': 'on_chat_model_end', 'name': 'BedrockChat', 'run_id': 'b94eb4ff-ec35-491f-91e0-a3d1365e2d11', 'tags': [], 'metadata': {}, 'data': {'input': {'messages': [[HumanMessage(content='tell me a short joke about AI research')]]}, 'output': {'generations': [[{'text': \"Why can't AI researchers tell the difference between Halloween and Christmas? Because Oct 31 = Dec 25!\", 'generation_info': None, 'type': 'ChatGeneration', 'message': AIMessage(content=\"Why can't AI researchers tell the difference between Halloween and Christmas? Because Oct 31 = Dec 25!\")}]], 'llm_output': None, 'run': None}}}\n",
      "----------------------------------------------------------------------\n",
      "{'event': 'on_parser_start', 'name': 'StrOutputParser', 'run_id': 'b188dca1-1468-49d0-b730-fb1bf1e43b24', 'tags': ['seq:step:4'], 'metadata': {}, 'data': {}}\n",
      "----------------------------------------------------------------------\n",
      "{'event': 'on_parser_stream', 'name': 'StrOutputParser', 'run_id': 'b188dca1-1468-49d0-b730-fb1bf1e43b24', 'tags': ['seq:step:4'], 'metadata': {}, 'data': {'chunk': \"Why can't AI researchers tell the difference between Halloween and Christmas? Because Oct 31 = Dec 25!\"}}\n",
      "----------------------------------------------------------------------\n",
      "{'event': 'on_chain_stream', 'run_id': 'e348e1ac-9a95-46a1-b74c-6ac75ffd7a16', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': \"Why can't AI researchers tell the difference between Halloween and Christmas? Because Oct 31 = Dec 25!\"}}\n",
      "----------------------------------------------------------------------\n",
      "{'event': 'on_chain_end', 'name': 'RunnableWithFallbacks', 'run_id': '2bae6484-b098-45dd-b583-06856b72ed9b', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'input': ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about AI research')]), 'output': AIMessage(content=\"Why can't AI researchers tell the difference between Halloween and Christmas? Because Oct 31 = Dec 25!\")}}\n",
      "----------------------------------------------------------------------\n",
      "{'event': 'on_parser_end', 'name': 'StrOutputParser', 'run_id': 'b188dca1-1468-49d0-b730-fb1bf1e43b24', 'tags': ['seq:step:4'], 'metadata': {}, 'data': {'input': AIMessage(content=\"Why can't AI researchers tell the difference between Halloween and Christmas? Because Oct 31 = Dec 25!\"), 'output': \"Why can't AI researchers tell the difference between Halloween and Christmas? Because Oct 31 = Dec 25!\"}}\n",
      "----------------------------------------------------------------------\n",
      "{'event': 'on_chain_end', 'name': 'RunnableSequence', 'run_id': 'e348e1ac-9a95-46a1-b74c-6ac75ffd7a16', 'tags': [], 'metadata': {}, 'data': {'output': \"Why can't AI researchers tell the difference between Halloween and Christmas? Because Oct 31 = Dec 25!\"}}\n"
     ]
    }
   ],
   "source": [
    "# response = chain.astream_log(\"AI research\", config={\"llm\": \"haiku\"})\n",
    "response = chain.astream_events(\"AI research\", config={\"llm\": \"haiku\"}, version=\"v1\")\n",
    "\n",
    "async for chunk in response:\n",
    "    print(\"-\" * 70)\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### streaming ðŸ¥·"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `stream` and `astream` - for final output from chain\n",
    "2. `astream_events`, `astream_log` - for both intermediate steps and final output from chain\n",
    "3. `Streaming` is only possible if all steps in the program know how to process an input stream; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n",
    "4. `Challenges` - (easy) to stream tokens produced by LLM -> (hard) to stream parts of JSON result before the entire JSON is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'countries': []}\n",
      "{'countries': [{}]}\n",
      "{'countries': [{'name': ''}]}\n",
      "{'countries': [{'name': 'France'}]}\n",
      "{'countries': [{'name': 'France', 'population': 67}]}\n",
      "{'countries': [{'name': 'France', 'population': 67059}]}\n",
      "{'countries': [{'name': 'France', 'population': 67059887}]}\n",
      "{'countries': [{'name': 'France', 'population': 67059887}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': 67059887}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': 67059887}, {'name': 'Spain'}]}\n",
      "{'countries': [{'name': 'France', 'population': 67059887}, {'name': 'Spain', 'population': 46}]}\n",
      "{'countries': [{'name': 'France', 'population': 67059887}, {'name': 'Spain', 'population': 46754}]}\n",
      "{'countries': [{'name': 'France', 'population': 67059887}, {'name': 'Spain', 'population': 46754778}]}\n",
      "{'countries': [{'name': 'France', 'population': 67059887}, {'name': 'Spain', 'population': 46754778}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': 67059887}, {'name': 'Spain', 'population': 46754778}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': 67059887}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan'}]}\n",
      "{'countries': [{'name': 'France', 'population': 67059887}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan', 'population': 125}]}\n",
      "{'countries': [{'name': 'France', 'population': 67059887}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan', 'population': 125836}]}\n",
      "{'countries': [{'name': 'France', 'population': 67059887}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan', 'population': 125836021}]}\n"
     ]
    }
   ],
   "source": [
    "### streaming a json response\n",
    "import boto3\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "haiku = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    client=boto3.client(\"bedrock-runtime\"),\n",
    "    model_kwargs={\"temperature\": 0.0, \"max_tokens\":128}\n",
    ")\n",
    "\n",
    "chain =  haiku | JsonOutputParser()\n",
    "# Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\n",
    "\n",
    "input_text = 'output a list of the countries france, spain and japan and their populations in JSON format. Do not explain.  Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'\n",
    "# async for text in chain.astream(input_text):\n",
    "for text in chain.stream(input_text):\n",
    "    print(text, flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France', 'Spain', 'Japan']|"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import (\n",
    "    JsonOutputParser,\n",
    ")\n",
    "\n",
    "\n",
    "# A function that operates on finalized inputs\n",
    "# rather than on an input_stream\n",
    "def _extract_country_names(inputs):\n",
    "    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\n",
    "    if not isinstance(inputs, dict):\n",
    "        return \"\"\n",
    "\n",
    "    if \"countries\" not in inputs:\n",
    "        return \"\"\n",
    "\n",
    "    countries = inputs[\"countries\"]\n",
    "\n",
    "    if not isinstance(countries, list):\n",
    "        return \"\"\n",
    "\n",
    "    country_names = [\n",
    "        country.get(\"name\") for country in countries if isinstance(country, dict)\n",
    "    ]\n",
    "    return country_names\n",
    "\n",
    "\n",
    "chain = haiku | JsonOutputParser() | _extract_country_names\n",
    "\n",
    "async for text in chain.astream(\n",
    "    'output a list of the countries france, spain and japan and their populations in JSON format. Do not explain.  Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'\n",
    "):\n",
    "    print(text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France|Spain|Japan|"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "async def _extract_country_names_streaming(input_stream):\n",
    "    \"\"\"A function that operates on input streams.\"\"\"\n",
    "    country_names_so_far = set()\n",
    "\n",
    "    async for input in input_stream:\n",
    "        if not isinstance(input, dict):\n",
    "            continue\n",
    "\n",
    "        if \"countries\" not in input:\n",
    "            continue\n",
    "\n",
    "        countries = input[\"countries\"]\n",
    "\n",
    "        if not isinstance(countries, list):\n",
    "            continue\n",
    "\n",
    "        for country in countries:\n",
    "            name = country.get(\"name\")\n",
    "            if not name:\n",
    "                continue\n",
    "            if name not in country_names_so_far:\n",
    "                yield name\n",
    "                country_names_so_far.add(name)\n",
    "\n",
    "\n",
    "chain = haiku | JsonOutputParser() | _extract_country_names_streaming\n",
    "\n",
    "async for text in chain.astream(\n",
    "    'output a list of the countries france, spain and japan and their populations in JSON format. Do not explain.  Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'\n",
    "):\n",
    "    print(text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. RunnableParallel\n",
    "2. RunnablePassthrough\n",
    "3. RunnableLambda\n",
    "4. RunnableConfig\n",
    "5. RunnableBranch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accepting a runnable config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_or_fix(\n",
    "    text: str, \n",
    "    config: RunnableConfig\n",
    "):\n",
    "    # print(f\"text: {text}\")\n",
    "    fix_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\\n\\nHuman: Fix the following text:\\n\\n```text\\n{input}\\n```\\nError: {error}\\n\"\n",
    "        \"Don't narrate, just respond with the fixed data. \\nAssistant: \"\n",
    "    )\n",
    "\n",
    "    llm = BedrockChat(\n",
    "        # model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        client=boto3.client(\"bedrock-runtime\"),\n",
    "        model_kwargs={\"temperature\": 0.0, \"max_tokens\":128}\n",
    "    )\n",
    "\n",
    "    fixing_chain = fix_prompt | llm | StrOutputParser()\n",
    "\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except Exception as e:\n",
    "            # print(f\"Error: {e}\")\n",
    "            text = fixing_chain.invoke(\n",
    "                {\n",
    "                    \"input\": text,\n",
    "                    \"error\": e\n",
    "                },\n",
    "                # config\n",
    "            )\n",
    "\n",
    "    return \"Failed to parse\"\n",
    "\n",
    "# result = parse_or_fix(\n",
    "#     text=\"{foo: bar}\",\n",
    "#     config={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'foo': 'bar'}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'foo': 'bar'}\n",
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# to use get_openai_callback - replace the llm with ChatOpenAI()\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    output = RunnableLambda(parse_or_fix).invoke(\n",
    "        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}\n",
    "    )\n",
    "    print(output)\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dynamically route logic based on input"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "qn -> classify_topic -> decide_chain_type\n",
    "                            /     |     \\\n",
    "                        haiku  sonnet  general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Haiku'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "claude2 = BedrockChat(\n",
    "    model_id=\"anthropic.claude-v2:1\",\n",
    "    client=boto3.client(\"bedrock-runtime\"),\n",
    "    model_kwargs={\"temperature\": 0.0, \"max_tokens\": 128}\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Given the user question below, classify it as either being about `Haiku`, `Sonnet`, or `Other`.\n",
    "\n",
    "Do not respond with more than one word.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\"\"\"\n",
    "    )\n",
    "    | claude2\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"how do I call Haiku?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "haiku = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    client=boto3.client(\"bedrock-runtime\"),\n",
    "    model_kwargs={\"temperature\": 0.0, \"max_tokens\":128}\n",
    ")\n",
    "\n",
    "sonnet  = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    client=boto3.client(\"bedrock-runtime\"),\n",
    "    model_kwargs={\"temperature\": 0.0, \"max_tokens\": 128}\n",
    ")\n",
    "\n",
    "claude2 = BedrockChat(\n",
    "    model_id=\"anthropic.claude-v2:1\",\n",
    "    client=boto3.client(\"bedrock-runtime\"),\n",
    "    model_kwargs={\"temperature\": 0.0, \"max_tokens\": 128}\n",
    ")\n",
    "\n",
    "\n",
    "haiku_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"You are an AI research assistant called Haiku. \\\n",
    "Always answer questions starting with \"Haiku:\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    | haiku\n",
    ")\n",
    "\n",
    "sonnet_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "         \"\"\"You are an AI research assistant called Sonnet. \\\n",
    "Always answer questions starting with \"Sonnet:\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    | sonnet\n",
    ")\n",
    "general_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    | claude2\n",
    ")\n",
    "\n",
    "\n",
    "def route(info):\n",
    "    if \"haiku\" in info[\"topic\"].lower():\n",
    "        return haiku_chain\n",
    "    elif \"sonnet\" in info[\"topic\"].lower():\n",
    "        return sonnet_chain\n",
    "    else:\n",
    "        return general_chain \n",
    "\n",
    "full_chain = (\n",
    "    {\n",
    "        \"topic\": chain,\n",
    "        \"question\": lambda x: x[\"question\"]\n",
    "    }\n",
    "    | RunnableLambda(route)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here are a few tips for using Anthropic:\\n\\n- Ask Claude open-ended questions or have conversations. Claude is designed to have natural conversations and be helpful, harmless, and honest. Simply ask questions or make statements to have a dialogue.\\n\\n- Provide context and clarification when needed. If Claude seems confused or gives an unhelpful response, try rephrasing your question or providing more details to guide the conversation. \\n\\n- Adjust your expectations. Claude has limitations in its knowledge and capabilities. Expect insightful but simplistic responses rather than fully comprehensive expertise.\\n\\n- Check the website and documentation.')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"how do I use Anthropic?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Haiku: To use Anthropic Haiku, simply ask me questions and I will respond in the form of a haiku poem. I am an AI research assistant created by Anthropic to provide helpful information in a concise, poetic format. Feel free to ask me anything, and I will do my best to answer in a thoughtful, 3-line haiku.')"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"how do I use Anthropic Haiku?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sonnet: As an AI research assistant, I can assist you with various tasks related to your research projects. Some ways I can help include literature review, data analysis, writing and editing research papers, creating visualizations, and answering specific questions within my knowledge domain. Please provide more details about your research area and specific needs, and I'll do my best to support you effectively.\")"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"how do I use Anthropic Sonnet for research?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='4')"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"whats 2 + 2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using a RunnableBranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: \"haiku\" in x[\"topic\"].lower(), haiku_chain),\n",
    "    (lambda x: \"sonnet\" in x[\"topic\"].lower(), sonnet_chain),\n",
    "    general_chain,\n",
    ")\n",
    "\n",
    "full_chain = (\n",
    "    {\n",
    "        \"topic\": chain,\n",
    "        \"question\": lambda x: x[\"question\"]\n",
    "    }\n",
    "    | branch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Haiku: Anthropic's Haiku is a powerful AI research tool that can assist with advanced AI research, including work on artificial general intelligence (AGI). To use Haiku for AGI research, I recommend the following steps:\\n\\n1. Familiarize yourself with Haiku's capabilities and features. Explore the documentation and experiment with the tool to understand how it can support your research.\\n\\n2. Identify specific areas of your AGI research where Haiku could be beneficial, such as data processing, model training, or analysis of results.\\n\\n3. Integrate Haiku into\")"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\n",
    "    \"question\": \"how to use Anthropic's Haiku to work on AGI research?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(nodes={'39ba451d9f274f8c8484fe5a4b661f3f': Node(id='39ba451d9f274f8c8484fe5a4b661f3f', data=<class 'pydantic.main.RunnableParallel<topic,question>Input'>), '4d3b56a918c64317a5e863b64808ea1c': Node(id='4d3b56a918c64317a5e863b64808ea1c', data=<class 'pydantic.main.RunnableParallel<topic,question>Output'>), '40f95b98b41e400691f43883b4e402f9': Node(id='40f95b98b41e400691f43883b4e402f9', data=PromptTemplate(input_variables=['question'], template='Given the user question below, classify it as either being about `Haiku`, `Sonnet`, or `Other`.\\n\\nDo not respond with more than one word.\\n\\n<question>\\n{question}\\n</question>\\n\\nClassification:')), '4e16a4a65a2b43828e3d73261a15d0c8': Node(id='4e16a4a65a2b43828e3d73261a15d0c8', data=BedrockChat(client=<botocore.client.BedrockRuntime object at 0x7f9a2397f2e0>, model_id='anthropic.claude-v2:1', model_kwargs={'temperature': 0.0, 'max_tokens': 128})), '096bed4f6f564d3f9868c4db32b3a830': Node(id='096bed4f6f564d3f9868c4db32b3a830', data=StrOutputParser()), '7ec64ec648274435bdc81f9316295340': Node(id='7ec64ec648274435bdc81f9316295340', data=RunnableLambda(...)), '366d88c7632b45f4964cf715e7ff3630': Node(id='366d88c7632b45f4964cf715e7ff3630', data=RunnableBranch(branches=[(RunnableLambda(lambda x: 'haiku' in x['topic'].lower()), PromptTemplate(input_variables=['question'], template='You are an AI research assistant called Haiku. Always answer questions starting with \"Haiku:\". Respond to the following question:\\n\\nQuestion: {question}\\nAnswer:')\n",
       "| BedrockChat(client=<botocore.client.BedrockRuntime object at 0x7f9a2397e8f0>, model_id='anthropic.claude-3-haiku-20240307-v1:0', model_kwargs={'temperature': 0.0, 'max_tokens': 128})), (RunnableLambda(lambda x: 'sonnet' in x['topic'].lower()), PromptTemplate(input_variables=['question'], template='You are an AI research assistant called Sonnet. Always answer questions starting with \"Sonnet:\". Respond to the following question:\\n\\nQuestion: {question}\\nAnswer:')\n",
       "| BedrockChat(client=<botocore.client.BedrockRuntime object at 0x7f9a238437c0>, model_id='anthropic.claude-3-sonnet-20240229-v1:0', model_kwargs={'temperature': 0.0, 'max_tokens': 128}))], default=PromptTemplate(input_variables=['question'], template='Respond to the following question:\\n\\nQuestion: {question}\\nAnswer:')\n",
       "| BedrockChat(client=<botocore.client.BedrockRuntime object at 0x7f9a23841690>, model_id='anthropic.claude-v2:1', model_kwargs={'temperature': 0.0, 'max_tokens': 128}))), '9b8639ebc46444ac8117fe3cb67bc2ca': Node(id='9b8639ebc46444ac8117fe3cb67bc2ca', data=<class 'pydantic.main.RunnableBranchOutput'>)}, edges=[Edge(source='40f95b98b41e400691f43883b4e402f9', target='4e16a4a65a2b43828e3d73261a15d0c8', data=None), Edge(source='4e16a4a65a2b43828e3d73261a15d0c8', target='096bed4f6f564d3f9868c4db32b3a830', data=None), Edge(source='39ba451d9f274f8c8484fe5a4b661f3f', target='40f95b98b41e400691f43883b4e402f9', data=None), Edge(source='096bed4f6f564d3f9868c4db32b3a830', target='4d3b56a918c64317a5e863b64808ea1c', data=None), Edge(source='39ba451d9f274f8c8484fe5a4b661f3f', target='7ec64ec648274435bdc81f9316295340', data=None), Edge(source='7ec64ec648274435bdc81f9316295340', target='4d3b56a918c64317a5e863b64808ea1c', data=None), Edge(source='366d88c7632b45f4964cf715e7ff3630', target='9b8639ebc46444ac8117fe3cb67bc2ca', data=None), Edge(source='4d3b56a918c64317a5e863b64808ea1c', target='366d88c7632b45f4964cf715e7ff3630', data=None)])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.get_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ackaging (/opt/conda/envs/pytorch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ackaging (/opt/conda/envs/pytorch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m       +-------------------------------+       \n",
      "       | Parallel<topic,question>Input |       \n",
      "       +-------------------------------+       \n",
      "                **            ***              \n",
      "              **                 **            \n",
      "            **                     **          \n",
      "+----------------+                   **        \n",
      "| PromptTemplate |                    *        \n",
      "+----------------+                    *        \n",
      "          *                           *        \n",
      "          *                           *        \n",
      "          *                           *        \n",
      "  +-------------+                     *        \n",
      "  | BedrockChat |                     *        \n",
      "  +-------------+                     *        \n",
      "          *                           *        \n",
      "          *                           *        \n",
      "          *                           *        \n",
      "+-----------------+           +-------------+  \n",
      "| StrOutputParser |           | Lambda(...) |  \n",
      "+-----------------+           +-------------+  \n",
      "                **            **               \n",
      "                  **        **                 \n",
      "                    **    **                   \n",
      "      +--------------------------------+       \n",
      "      | Parallel<topic,question>Output |       \n",
      "      +--------------------------------+       \n",
      "                        *                      \n",
      "                        *                      \n",
      "                        *                      \n",
      "                  +--------+                   \n",
      "                  | Branch |                   \n",
      "                  +--------+                   \n",
      "                        *                      \n",
      "                        *                      \n",
      "                        *                      \n",
      "                +--------------+               \n",
      "                | BranchOutput |               \n",
      "                +--------------+               \n"
     ]
    }
   ],
   "source": [
    "# ! pip install grandalf --quiet\n",
    "\n",
    "full_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PromptTemplate(input_variables=['question'], template='Given the user question below, classify it as either being about `Haiku`, `Sonnet`, or `Other`.\\n\\nDo not respond with more than one word.\\n\\n<question>\\n{question}\\n</question>\\n\\nClassification:')]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.get_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "purpose: add message history to chain\n",
    "\n",
    "how: wrap Runnable with another that can manage the chat message history -> RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_models import  BedrockChat\n",
    "import boto3\n",
    "\n",
    "llm = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    client=boto3.client(\"bedrock-runtime\"),\n",
    "    model_kwargs={\"temperature\": 0.0, \"max_tokens\":128}\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You're an assistant who's good at {ability}. Respond in 20 words or fewer\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"{question}\"\n",
    "    )\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Cosine is the trigonometric function that represents the ratio of the adjacent side to the hypotenuse of a right triangle.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke(\n",
    "    {\n",
    "        \"ability\": \"math\",\n",
    "        \"question\": \"What does cosine mean?\"\n",
    "    },\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"session_id\": \"001\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Cosine is the ratio of the adjacent side to the hypotenuse. Examples: cos(0Â°) = 1, cos(90Â°) = 0, cos(45Â°) = âˆš2/2.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke(\n",
    "    {\n",
    "        \"ability\": \"math\",\n",
    "        \"question\": \"Simplify it and give examples\"\n",
    "    },\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"session_id\": \"001\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Simplify mathematical expressions, provide step-by-step solutions, and give numerical examples.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke(\n",
    "    {\n",
    "        \"ability\": \"math\",\n",
    "        \"question\": \"Simplify it and give examples\"\n",
    "    },\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"session_id\": \"002\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
